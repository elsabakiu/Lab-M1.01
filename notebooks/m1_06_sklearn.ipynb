{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sklearn Machine Learning Examples\n",
        "\n",
        "This notebook demonstrates classification, regression, dimensionality reduction, and clustering using sklearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Classification Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold  # Correct import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.0)\n",
        "X_temp = df.drop('Survived', axis=1)\n",
        "selector.fit(X_temp)\n",
        "selected_features = X_temp.columns[selector.get_support()]\n",
        "df = df[list(selected_features) + ['Survived']]\n",
        "\n",
        "correlation_matrix = df.drop('Survived', axis=1).corr().abs()\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "df = df.drop(columns=high_corr_features)\n",
        "\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, lr_pred),\n",
        "        accuracy_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'Precision': [\n",
        "        precision_score(y_test, lr_pred),\n",
        "        precision_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_score(y_test, lr_pred),\n",
        "        recall_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_score(y_test, lr_pred),\n",
        "        f1_score(y_test, rf_pred)\n",
        "    ],\n",
        "    'AUC-ROC': [\n",
        "        roc_auc_score(y_test, lr_pred_proba),\n",
        "        roc_auc_score(y_test, rf_pred_proba)\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(metrics)\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Regression Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold  \n",
        "\n",
        "house_df = pd.read_csv('https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv')\n",
        "house_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_df = pd.get_dummies(house_df, columns=['ocean_proximity'], drop_first=True)\n",
        "\n",
        "house_df = house_df.select_dtypes(include=[np.number])\n",
        "\n",
        "selector_house = VarianceThreshold(threshold=0.0)\n",
        "X_temp_house = house_df.drop('median_house_value', axis=1)\n",
        "selector_house.fit(X_temp_house)\n",
        "selected_features_house = X_temp_house.columns[selector_house.get_support()]\n",
        "house_df = house_df[list(selected_features_house) + ['median_house_value']]\n",
        "\n",
        "correlation_matrix = house_df.drop('median_house_value', axis=1).corr().abs()\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "house_df = house_df.drop(columns=high_corr_features)\n",
        "\n",
        "X_house = house_df.drop('median_house_value', axis=1)\n",
        "y_house = house_df['median_house_value']\n",
        "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
        "    X_house, y_house, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 KNN Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "knn_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('power_transformer', PowerTransformer(method='yeo-johnson')),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_pipeline.fit(X_train_house, y_train_house)\n",
        "knn_pred = knn_pipeline.predict(X_test_house)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_lr = X_train_house.fillna(X_train_house.median())\n",
        "X_test_lr = X_test_house.fillna(X_train_house.median())\n",
        "\n",
        "scaler_lr = StandardScaler()\n",
        "X_train_lr_scaled = scaler_lr.fit_transform(X_train_lr)\n",
        "X_test_lr_scaled = scaler_lr.transform(X_test_lr)\n",
        "\n",
        "correlation_matrix_lr = pd.DataFrame(X_train_lr_scaled).corr().abs()\n",
        "upper_triangle_lr = correlation_matrix_lr.where(\n",
        "    np.triu(np.ones(correlation_matrix_lr.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features_lr = [column for column in upper_triangle_lr.columns if any(upper_triangle_lr[column] > 0.95)]\n",
        "X_train_lr_scaled = pd.DataFrame(X_train_lr_scaled).drop(columns=high_corr_features_lr).values\n",
        "X_test_lr_scaled = pd.DataFrame(X_test_lr_scaled).drop(columns=high_corr_features_lr).values\n",
        "\n",
        "lr_reg_model = LinearRegression()\n",
        "lr_reg_model.fit(X_train_lr_scaled, y_train_house)\n",
        "lr_reg_pred = lr_reg_model.predict(X_test_lr_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "regression_metrics = {\n",
        "    'Model': ['KNN Regression', 'Linear Regression'],\n",
        "    'RMSE': [\n",
        "        np.sqrt(mean_squared_error(y_test_house, knn_pred)),\n",
        "        np.sqrt(mean_squared_error(y_test_house, lr_reg_pred))\n",
        "    ],\n",
        "    'MAE': [\n",
        "        mean_absolute_error(y_test_house, knn_pred),\n",
        "        mean_absolute_error(y_test_house, lr_reg_pred)\n",
        "    ],\n",
        "    'R2': [\n",
        "        r2_score(y_test_house, knn_pred),\n",
        "        r2_score(y_test_house, lr_reg_pred)\n",
        "    ],\n",
        "    'MAPE': [\n",
        "        mean_absolute_percentage_error(y_test_house, knn_pred),\n",
        "        mean_absolute_percentage_error(y_test_house, lr_reg_pred)\n",
        "    ]\n",
        "}\n",
        "\n",
        "regression_comparison_df = pd.DataFrame(regression_metrics)\n",
        "print(regression_comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion: are these a good models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PCA with Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca_df = house_df.copy()\n",
        "X_pca = pca_df.drop('median_house_value', axis=1)\n",
        "X_pca = X_pca.fillna(X_pca.median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Normalization and PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler_pca = StandardScaler()\n",
        "X_pca_scaled = scaler_pca.fit_transform(X_pca)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca_transformed = pca.fit_transform(X_pca_scaled)\n",
        "\n",
        "print(f\"Original features: {X_pca_scaled.shape[1]}\")\n",
        "print(f\"PCA components (95% variance): {X_pca_transformed.shape[1]}\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Variance Explained Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca_2d = pca_2d.fit_transform(X_pca_scaled)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=30)\n",
        "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
        "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
        "axes[0].set_title('PCA - First Two Principal Components')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "\n",
        "n_components_to_show = min(10, len(pca.explained_variance_ratio_))\n",
        "axes[1].bar(range(1, n_components_to_show + 1), \n",
        "            pca.explained_variance_ratio_[:n_components_to_show])\n",
        "axes[1].set_xlabel('Principal Component')\n",
        "axes[1].set_ylabel('Explained Variance Ratio')\n",
        "axes[1].set_title('Explained Variance by Component')\n",
        "axes[1].set_xticks(range(1, n_components_to_show + 1))\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "cumsum_var = np.cumsum(pca.explained_variance_ratio_[:n_components_to_show])\n",
        "ax2 = axes[1].twinx()\n",
        "ax2.plot(range(1, n_components_to_show + 1), cumsum_var, 'r-', marker='o', label='Cumulative')\n",
        "ax2.set_ylabel('Cumulative Explained Variance', color='r')\n",
        "ax2.tick_params(axis='y', labelcolor='r')\n",
        "ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='95% threshold')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Variance explained by PC1: {pca_2d.explained_variance_ratio_[0]:.4f}\")\n",
        "print(f\"Variance explained by PC2: {pca_2d.explained_variance_ratio_[1]:.4f}\")\n",
        "print(f\"Total variance (first 2 components): {pca_2d.explained_variance_ratio_.sum():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. K-Means Clustering with Elbow Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_selection import VarianceThreshold  \n",
        "\n",
        "kmeans_df = house_df.copy()\n",
        "X_kmeans = kmeans_df.drop('median_house_value', axis=1)\n",
        "X_kmeans = X_kmeans.fillna(X_kmeans.median())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_kmeans = X_kmeans.select_dtypes(include=[np.number])\n",
        "\n",
        "selector_km = VarianceThreshold(threshold=0.0)\n",
        "selector_km.fit(X_kmeans)\n",
        "selected_features_km = X_kmeans.columns[selector_km.get_support()]\n",
        "X_kmeans = X_kmeans[selected_features_km]\n",
        "\n",
        "correlation_matrix_km = X_kmeans.corr().abs()\n",
        "upper_triangle_km = correlation_matrix_km.where(\n",
        "    np.triu(np.ones(correlation_matrix_km.shape), k=1).astype(bool)\n",
        ")\n",
        "high_corr_features_km = [column for column in upper_triangle_km.columns if any(upper_triangle_km[column] > 0.95)]\n",
        "X_kmeans = X_kmeans.drop(columns=high_corr_features_km)\n",
        "\n",
        "scaler_km = StandardScaler()\n",
        "transformer_km = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "X_kmeans_scaled = scaler_km.fit_transform(X_kmeans)\n",
        "X_kmeans_transformed = transformer_km.fit_transform(X_kmeans_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Elbow Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inertias = []\n",
        "K_range = range(1, 20)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_kmeans_transformed)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertias, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Final K-Means Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimal_k = 2\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters = final_kmeans.fit_predict(X_kmeans_transformed)\n",
        "\n",
        "silhouette_avg = silhouette_score(X_kmeans_transformed, clusters)\n",
        "print(f\"Optimal k: {optimal_k}\")\n",
        "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(f\"Cluster sizes: {np.bincount(clusters)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_kmeans['Cluster'] = clusters\n",
        "\n",
        "cluster_summary = X_kmeans.groupby('Cluster').agg(['mean', 'std', 'count'])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLUSTER CHARACTERISTICS - Mean Values\")\n",
        "print(\"=\" * 80)\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"\\n--- Cluster {cluster_id} (n={np.sum(clusters == cluster_id)}) ---\")\n",
        "    cluster_means = X_kmeans[X_kmeans['Cluster'] == cluster_id].drop('Cluster', axis=1).mean()\n",
        "    print(cluster_means.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cluster_profiles = X_kmeans.groupby('Cluster').mean()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "cluster_profiles_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(cluster_profiles.T).T,\n",
        "    index=cluster_profiles.index,\n",
        "    columns=cluster_profiles.columns\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, ax=axes[0], cbar_kws={'label': 'Mean Value'})\n",
        "axes[0].set_title('Cluster Profiles - Raw Mean Values')\n",
        "axes[0].set_xlabel('Cluster')\n",
        "axes[0].set_ylabel('Features')\n",
        "\n",
        "# Heatmap 2: Standardized values (z-scores)\n",
        "sns.heatmap(cluster_profiles_scaled.T, annot=True, fmt='.2f', cmap='RdBu_r', \n",
        "            center=0, ax=axes[1], cbar_kws={'label': 'Z-Score'})\n",
        "axes[1].set_title('Cluster Profiles - Standardized Values (Z-scores)')\n",
        "axes[1].set_xlabel('Cluster')\n",
        "axes[1].set_ylabel('Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEATURE IMPORTANCE FOR CLUSTERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "feature_importance = cluster_profiles.var(axis=0).sort_values(ascending=False)\n",
        "print(\"\\nFeatures with highest variance across clusters (most discriminative):\")\n",
        "print(feature_importance)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETAILED CLUSTER COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "comparison_df = pd.DataFrame()\n",
        "for i in range(optimal_k):\n",
        "    comparison_df[f'Cluster_{i}'] = X_kmeans[X_kmeans['Cluster'] == i].drop('Cluster', axis=1).mean()\n",
        "\n",
        "comparison_df = comparison_df.T\n",
        "print(comparison_df.round(2))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DISTINCTIVE FEATURES PER CLUSTER\")\n",
        "print(\"=\" * 80)\n",
        "for cluster_id in range(optimal_k):\n",
        "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
        "    cluster_data = cluster_profiles_scaled.loc[cluster_id].sort_values(ascending=False)\n",
        "    high_features = cluster_data[cluster_data > 1.5].index.tolist()\n",
        "    low_features = cluster_data[cluster_data < -1.5].index.tolist()\n",
        "    \n",
        "    if high_features:\n",
        "        print(f\"Notably HIGH: {', '.join(high_features)}\")\n",
        "    if low_features:\n",
        "        print(f\"Notably LOW: {', '.join(low_features)}\")\n",
        "    if not high_features and not low_features:\n",
        "        print(\"No extremely distinctive features (moderate values)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_viz = PCA(n_components=2)\n",
        "X_pca_viz = pca_viz.fit_transform(X_kmeans_transformed)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], \n",
        "                     c=clusters, cmap='viridis', alpha=0.6, s=50)\n",
        "plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.title('K-Means Clusters in PCA Space')\n",
        "plt.colorbar(scatter, label='Cluster', ticks=range(optimal_k))\n",
        "\n",
        "# Add cluster centers\n",
        "centers_pca = pca_viz.transform(final_kmeans.cluster_centers_)\n",
        "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
        "           c='red', marker='X', s=200, edgecolors='black', linewidth=2,\n",
        "           label='Centroids')\n",
        "\n",
        "# Add cluster labels\n",
        "for i, (x, y) in enumerate(centers_pca):\n",
        "    plt.annotate(f'C{i}', (x, y), fontsize=12, fontweight='bold',\n",
        "                ha='center', va='center', color='white')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Clean up - remove the cluster column if you want to keep original data intact\n",
        "X_kmeans = X_kmeans.drop('Cluster', axis=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}